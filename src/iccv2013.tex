\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{A Unified Framework for Recognizing Discrete and Continuous Flow Gestures}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

\subsection{Related work}
\subsubsection{Gesture taxonomy}
Manipulative vs. communicative and continuous vs. discrete. How they are related
to each other. 

Wobbrock \etal developed a taxonomy of surface gestures based on a user-defined
gesture set~\cite{Wobbrock09}. They classified each gesture along four dimensions: 
\textit{form}, \textit{nature}, \textit{binding}, and \textit{flow}. Their work 
provides good guidelines for designing a surface gesture interaction system. 
We draw on their gesture taxonomy and design guidelines in developing our gesture 
interaction system. 

\subsubsection{Gesture recognition}
Previous work on gesture recognition have been focusing on discrete gestures of either
static or dynamic forms. Hidden Markov model (HMM) is a commonly used technique
in recognizing this type of gestures following its success in speech recognition. 
One HMM model is trained for each gesture class resulting in a mixture of HMMs (citation). During
recognition, for a given pre-segmented gesture sequence, the HMM that gives the highest
likelihood of the observed sequence is selected as the gesture class. One can also
connect the HMMs together to create an ``embeded HMM'' (citation). However in an
embeded HMM, the ending ``time'' of each sub-HMM is known in advance~\cite{murphy02}.
This makes online inference in real-time difficult because the gestures are not
pre-segmented. 

Some researchers also used discriminative models such as hidden conditional 
random fields (HCRF)~\cite{wang06} and latent-dynamic conditional random field (LDCRF)~\cite{morency07} provide some 
improvement on top of HMM. Note that HCRF is similar to the mixture of HMMs model except that
HCRF is an undirected graph and the mixture of HMMs is a directed graphs. Hence HCRF
can only handle segmented gesture sequences.

Prior work on handling both discrete and continuous flow gestures are limit and 
usually involve explicit mode change on the user part. Oka \etal developed a 
system that distinguishes a gesture as either continuous or discrete, in their 
terms manipulative or symbolic, according to the extension the extension of the 
thumb~\cite{Oka02}. They regard gestures with an extended thumb as direct manipulation
and those with a bent thumb as symbolic gestures. For direct manipulation, the
system selects operating modes such as rotate, move or resize based on the fingertips
configuration; for symbolic gestures, it uses HMM for classification. The way they
distinguish continuous and discrete flow gestures seems to be arbitrary and ``unnatural''.
As found by Wobbrock \etal, users rarely care about the number of fingers they 
employ~\cite{Wobbrock09} which disagrees with many designer-defined gesture set
~\cite{Malik05, Morris06, Rekimoto02, Tse06}. Other mode change methods may have the
user perform a grab gesture to start the continuous flow mode and a release gesture
to end the continuous flow mode. This is based on the observation of the demo video
of the work by Kim \etal~\cite{Kim12} although they did not explicitly describe this
in the paper. In the photo browsing system developed by Madhvanath \etal~\cite{Madhvanath12},
users use two hands to start the zoom action and the system thus switches to continuous
mode allowing users to pan the phone as well. The latter methods are more natural but may still
be limited and constrained. Even from a system implementation point view, it may 
involve specifying explicit conditions for mode switching (error prone, less robust
to make hard decisions?).   

It has also been established that three phases make a gesture: preparation, nucleus 
(peak or stroke~\cite{Mcneil82}), and retraction~\cite{Pavlovic97}. The preparation phase consists
of a preparatory movement that sets the hand in motion from some resting position.
The nucleus of a gesture has some ``definite form and enhanced dynamic qualities''
~\cite{kendon86}. Finally, the hand either returns to the resting position or repositions
for the new gesture phase. Most previous work for recognizing discrete flow gestures do not
distinguish the preparation and retraction phases (citations) from the whole gesture
sequences. There are also some systems that do make the distinction such as the work
by Sharma \etal~\cite{sharma00}. While it may not be necessary treat the preparation
and retraction phases separately for discrete flow gestures, we believe it is 
necessary for a system that also handles the continuous flow gestures because
the system needs to respond immediately when the nucleus of a continuous flow
gesture starts while nothing should be affected during the preparation and retraction
phases.   

\section{Framework for Recognizing Discrete and Continuous Flow Gestures}
Among the four dimensions in which Wobbrock \etal classified the surface gestures,
the \textit{flow} dimension is particularly relevant from a system development
point view because it refers to how the system should respond to users' acts. 
There are two categories in this dimension: \textit{discrete} or 
\textit{continuous}. A gesture's \textit{flow} is discrete if the system response
should occur \textit{after} the user acts; it is continuous if the system response
should occur \textit{while} the user acts!\cite{Wobbrock09}. In their user elicited
gesture set of 1080 gestures from 20 participants, the discrete and continuous flow 
gestures are about half-and-half.

This means that it is necessary for the system to differentiate discrete and continuous
flow gestures so that it can respond to the gestures accordingly. If a gesture's flow
is discrete, the whole sequence needs to be delimited, recognized, and responded
as a single event. Most of the communicative gestures fall into this category. 
For example, a \textit{wave} gesture that can mean ``no'' or getting the attention of the 
system should be responded at the end of the gesture. On the other hand, if the 
flow is continuous, ongoing system response is required. Most of the manipulative 
gestures fall into this category, such as \textit{pan} and \textit{resize} gestures. 
As the user moves his/her hand(s), the system has to respond in each frame such that
certain parameter(s) of the virtual object(s) that the user is controlling are tied to
certain parameter(s) of the user's hand(s).

One major distinction between the discrete and continuous flow gestures is that,
in a pre-defined gesture set, discrete flow gestures usually have specific hand
poses and movement paths, whereas continuous flow gestures usually do not have
specific movement paths and the hand poses are not as strictly defined. Consider 
the \textit{pan} and \textit{resize} gestures. The user's hand(s) can move in any
directions depending on where he/she wants to move the virtual object. As a result, 
when training a gesture recognition system, we cannot just supply examples of the
\textit{pan} gesture in one direction as training examples for it.

To include continuous flow gestures in the recognition frame, we propose to have
one class for all the continuous flow gestures and different classes for different
discrete flow gestures. The training data for the continuous flow gesture class
should be examples of gestures moving in different directions to account for the 
variability in this class. We also have one class for each of the preparation and 
retraction phases to have an explicit model for both of them so that we can detect
the start of the nucleus of the continuous gesture. 

\subsection{Abstract Hidden Markov Model} 
We propose to use Abstract Hidden Markov Model for online gesture recognition.
(Graph compares AHMM and embeded HMM)
 
%------------------------------------------------------------------------
\section{Experimental Evaluation}

We collected gesture data from x users. Each user performs a series of gestures 
including both continuous and discrete gestures. The continuous gestures include
panning to the right, panning to the left, rotating clockwise. Each are repeated once. 
The discrete gestures include thumb up, stop, and wave. The users
perform the gestures following an example shown to them. Each discrete gestures are 
repeated 3 times. The order of the gestures performed are random. We perform user
dependent recognition. The result will be relevant for building a system that can
recognize user dependent gesture input after the user supplies a few examples of 
each gesture.

Through the experimental evaluation, we want to answer the following questions: 1) Can we
distinguish continuous gestures from discrete gestures in a defined gesture set? 2) What are the relevant
features that help distinguish the two? 3) Can we detect the start and the end of 
continuous gestures reliably? 
  
%------------------------------------------------------------------------
\section{Conclusion and Future Work}
User independent evaluation and user adaptation on top of a user independent model.
We want to evaluate and compare different hand pose features such as HOG features 
with the eigenhand features we are using now.

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
